preprocess:
  max_len: 1024
  min_freq: 5
  vocab_path: '../data/bert.vocab'
  is_demo: 0

model:
  model_name: 'bert'
  load_checkpoint_path: ''
  load_wv_path: ''
  embed_size: 512    # in bert, embed_size = hidden_num

train:
  multi_gpu: 0
  batch_size: 16
  lr: 0.00001
  weight_decay: 0.01
  warmup_steps: 10000
  train_steps: 80000
  steps_per_check: 100
  steps_per_checkpoint: 80000

bert_model:
  max_position_embeddings: 1024
  num_hidden_layers: 6
  hidden_size: 512
  num_attention_heads: 8
  intermediate_size: 2048